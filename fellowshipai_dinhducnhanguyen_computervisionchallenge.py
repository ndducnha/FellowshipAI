# -*- coding: utf-8 -*-
"""FellowshipAI_DinhDucNhaNguyen_ComputerVisionChallenge.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sxmcsXHyx_EYq71m215gtcQ44tczK9VL
"""

# Fullname: Dinh Duc Nha Nguyen (Tony)
# Fellowship.ai: Computer Vision (CV) Challenge: Use a pre-trained ResNet 50 and train on the Flowers dataset.
# Date: 14/09/2024
# Email: ndducnha@gmail.com

# Import necessary libraries
import tensorflow as tf  # TensorFlow for building and training the model
from tensorflow.keras.models import Sequential  # Sequential model type for stacking layers
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D  # Layers to add on top of ResNet-50
from tensorflow.keras.preprocessing.image import ImageDataGenerator  # For loading and augmenting images
from tensorflow.keras.applications import ResNet50  # Pre-trained ResNet-50 model from Keras applications
import scipy.io  # For loading MATLAB files containing labels and dataset splits
import os  # For directory and file operations
import shutil  # For moving files to organized folders

# Step 1: Download and extract the dataset files
!wget -q http://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz  # Download flower images
!wget -q http://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat  # Download image labels
!wget -q http://www.robots.ox.ac.uk/~vgg/data/flowers/102/setid.mat  # Download data splits (train, val, test)
!tar -xvf 102flowers.tgz  # Extract the images from the tar file

# Step 2: Load the labels and data splits from the .mat files
labels = scipy.io.loadmat('imagelabels.mat')['labels'][0]  # Extract labels array (flower categories)
set_ids = scipy.io.loadmat('setid.mat')  # Load train, validation, and test split indices

# Extract indices for train, validation, and test sets, adjusted for 0-based indexing in Python
train_ids = set_ids['trnid'][0] - 1  # Training set indices
val_ids = set_ids['valid'][0] - 1  # Validation set indices
test_ids = set_ids['tstid'][0] - 1  # Test set indices

# Step 3: Create directories for organized data (train, validation, test)
os.makedirs('flower_data/train', exist_ok=True)
os.makedirs('flower_data/val', exist_ok=True)
os.makedirs('flower_data/test', exist_ok=True)

# Function to move images into respective folders based on data splits
def organize_data(image_ids, split_name):
    for idx in image_ids:  # Loop through each image index
        label = labels[idx]  # Get the label (class) for the current image
        img_name = f'image_{str(idx + 1).zfill(5)}.jpg'  # Image file naming format in the dataset
        class_folder = f'flower_data/{split_name}/{label}'  # Create folder path based on class label
        os.makedirs(class_folder, exist_ok=True)  # Create the class folder if it doesn't exist
        shutil.move(os.path.join('jpg', img_name), os.path.join(class_folder, img_name))  # Move image to the class folder

# Organize data into train, validation, and test directories
organize_data(train_ids, 'train')
organize_data(val_ids, 'val')
organize_data(test_ids, 'test')

# Step 4: Set up ImageDataGenerators for data augmentation and loading
train_datagen = ImageDataGenerator(
    rescale=1.0/255.0,  # Normalize pixel values to [0, 1]
    horizontal_flip=True,  # Randomly flip images horizontally
    zoom_range=0.2,  # Randomly zoom into images to augment data
    shear_range=0.2  # Randomly shear images to create more data variability
)

val_datagen = ImageDataGenerator(rescale=1.0/255.0)  # Only rescale for validation, no augmentation needed

# Load training images with augmentation and validation images without augmentation
train_generator = train_datagen.flow_from_directory(
    'flower_data/train',  # Directory for training images
    target_size=(224, 224),  # Resize images to 224x224 as required by ResNet-50
    batch_size=32,  # Number of images per batch
    class_mode='categorical'  # Multi-class classification mode (one-hot encoded)
)

val_generator = val_datagen.flow_from_directory(
    'flower_data/val',  # Directory for validation images
    target_size=(224, 224),  # Resize images to 224x224
    batch_size=32,  # Number of images per batch
    class_mode='categorical'  # Multi-class classification mode
)

# Step 5: Load the pre-trained ResNet-50 model without the top layers
base_model = ResNet50(
    weights='imagenet',  # Load weights pre-trained on the ImageNet dataset
    include_top=False,  # Exclude the top fully connected layers, use as a feature extractor
    input_shape=(224, 224, 3)  # Input shape for the model (224x224 RGB images)
)

# Freeze the base model layers to retain their pre-trained features
base_model.trainable = False

# Step 6: Create a Sequential model and add custom layers on top of the base model
model = Sequential([
    base_model,  # Add the pre-trained base model
    GlobalAveragePooling2D(),  # Add a global average pooling layer to reduce feature dimensions
    Dense(128, activation='relu'),  # Add a dense layer with 128 neurons and ReLU activation
    Dense(train_generator.num_classes, activation='softmax')  # Final layer for 102-class classification
])

# Compile the model with an optimizer, loss function, and evaluation metric
model.compile(
    optimizer='adam',  # Adam optimizer for adaptive learning rate
    loss='categorical_crossentropy',  # Loss function for multi-class classification
    metrics=['accuracy']  # Metric to evaluate model performance during training
)

# Step 7: Train the model using the training and validation data generators
history = model.fit(
    train_generator,  # Training data generator
    validation_data=val_generator,  # Validation data generator
    epochs=10  # Number of epochs (complete passes through the training data)
)

# Step 8: Save the trained model to a file for future use
model.save('resnet50_flowers_model.h5')  # Save the model weights and architecture